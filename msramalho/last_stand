# @msramalho

from __future__ import division
import sys
import os
import json
import random
import operator
import _thread as thread
from timeit import default_timer
from itertools import product
from copy import deepcopy
from collections import defaultdict
from math import log, sqrt
import datetime
import numpy as np
from time import sleep

import multipaint

#---------------------------------------------UTILS
sys.setrecursionlimit(3000)
random.seed()


def p(string=""):  # pragma: no cover
    print(string, file=sys.stderr, flush=True)


class Timer(object):  # pragma: no cover
    # https://gist.github.com/cgoldberg/2942781
    def __init__(self, name="", verbose=True):
        self.name = " for %s" % name
        self.verbose = verbose
        self.timer = default_timer

    def __enter__(self):
        self.start = self.timer()
        return self

    def __exit__(self, *args):
        end = self.timer()
        self.elapsed_secs = end - self.start
        self.elapsed = self.elapsed_secs * 1000  # millisecs
        if self.verbose:
            p("elapsed time%s: %f ms, %f s" % (self.name, self.elapsed, self.elapsed_secs))


possible_directions = [
    np.array([-1, -1]), np.array([-1,  0]), np.array([-1,  1]),
    np.array([0, -1]), np.array([0,  1]),
    np.array([1, -1]), np.array([1,  0]), np.array([1,  1])
]


def equal_move(m1, m2):
    return m1["type"] == m2["type"] and equal_dir(m1, m2)


def equal_dir(m1, m2):
    return np.array_equal(m1["direction"], m2["direction"])


def manhattan(p1, p2):
    return abs(p1[0] - p2[0]) + abs(p1[1] - p2[1])
#---------------------------------------------STATE
# height = number of rows
# width  = number of columns
# player position: {x: , y: } where x is the row (height) and y is the column (width)


class State:
    # me(my id), turns_left, rows, columns, players (dict of id=>(x,y)), board(ndarray), previous_actions (only if specified)
    def __init__(self, s, me, previous_actions=False):
        self.turns_left = int(s["turns_left"])
        self.rows = int(s["height"])
        self.columns = int(s["width"])

        # get numpy board from matrix of strings, must come before self.players due to self.player_ids
        self.loadBoard(s["colors"], s["player_positions"].keys())

        self.me = self.player_ids[me]

        # convert the json into a dict of {player1Name: (x, y), ...}
        self.players = {self.player_ids[p]: np.array([s["player_positions"][p][0], s["player_positions"][p][1]]) for p in s["player_positions"].keys()}

        # if last_move available
        if len(s["previous_actions"]) > 0:
            self.last_move = s["previous_actions"][-1]

        self.loadPlayerScores()
        self.loadMoves()

    # generate board - (-1 is for empty, 0 is for objects, 1 to \inf is for players' colors)
    def loadBoard(self, colors, player_names):
        self.player_ids = {}  # {-1: "neutral", 0: "object"}  # dict mapping player ids into names, useful to know a player from its number in the board
        temp = np.array(colors)
        condlist = [temp == None]
        choicelist = [-1]
        for i, player in enumerate(player_names):
            condlist.append(temp == player)
            choicelist.append(i + 1)
            self.player_ids[player] = i + 1  # update the player_ids dict
        self.board = np.select(condlist, choicelist)

    # given a game state (self) and a player id, not name, get the valid moves for that player
    def getValidMovesForPlayer(self, player):
        pos = self.players[player]  # (line, column) or (x, y)
        valid = []
        for d in possible_directions:
            if self.insideBoardAfterMove(pos, d):
                # t = d.tolist()  # temp, just so operation happens only once
                valid.extend([{"type": "shoot", "direction": d}, {"type": "walk", "direction": d}])
        return valid

    # get all my moves
    def loadMoves(self):
        self.moves = self.getValidMovesForPlayer(self.me)
        random.shuffle(self.moves)  # only needed for MC part
        self.fixed_moves = self.moves[:]

    # return True if the position p, after moving in direction d, is inside the board
    def insideBoard(self, p):
        return (0 <= p[0] and p[0] < self.rows) and (0 <= p[1] and p[1] < self.columns)

    # return True if the position p, after moving in direction d, is inside the board
    def insideBoardAfterMove(self, pos, direction):
        return self.insideBoard(pos + direction)

    # load self.scores with a dict of {player=>score}
    def loadPlayerScores(self):
        s1, s2 = np.unique(self.board, return_counts=True)  # unique, counts
        self.scores = dict(zip(s1, s2))

    # get the score for me
    def myScore(self):
        return self.scores[self.me]

    # get the score for my opponent(s)
    def opponentScore(self):
        return sum(self.scores[k] for k in self.scores if k > 0 and k != self.me)

    # get the position for me
    def myPos(self):
        return self.players[self.me]

    # get all the combinations of moves for all the players, for N players -> max is 16^N (256 for 2 players)
    def getAllMoveCombinations(self):
        return list(product(*[self.getValidMovesForPlayer(p) for p in self.players.keys()]))

    # given a position, a color, and a direction, count cells of the same color in the opposite direction
    def shotBackCount(self, pos, direction, color):
        res = 0
        bp = pos - direction  # back pos
        while self.insideBoard(bp) and self.board[bp[0], bp[1]] == color:
            bp -= direction  # back pos
            res += 1
        return max(res, 1)  # minimum shot size is 1

    # given a position, a direction, and the max value for it, calculate the distance to the edges [0---pos---end]
    def distanceToEnd(self, pos, direction, end):
        if direction == -1:  # backwards is the distance to the start, pos - 0
            return pos
        elif direction == 1:  # forwards is the distance to the end, end - pos
            return end - pos
        else:  # 0 means it stays put, return more than the max, in any direction
            return self.columns + self.rows

    # given a position and a direction return the shot range
    # minimum of the distance in each of the axis to the border
    def shotRange(self, pos, d):
        # had to take 1 from the rows and columns because it is zero indexed
        return min(self.distanceToEnd(pos[0], d[0], self.rows - 1), self.distanceToEnd(pos[1], d[1], self.columns - 1))

    # given a player and a direction get the range of the shot (max between shot load and board limits)
    def playerShotRange(self, player, d):
        return min(self.shotBackCount(self.players[player], d, player), self.shotRange(self.players[player], d))

    def getNewStateAlone(self, action):
        # create a copy of the current state
        state = deepcopy(self)
        # otherwise singleton does not work, because the moves were only valid for the previous state
        state.turns_left -= 1

        my_id = state.me
        if action["type"] == "walk":
            state.players[my_id] += action["direction"]  # sum pos with dir to give new pos
            pos = state.players[my_id]
            state.board[pos[0], pos[1]] = my_id  # update the board with the new positions
        else:  # shoot
            pos = state.players[my_id]
            for i in range(state.playerShotRange(my_id, action["direction"])):
                pos += action["direction"]  # one step in shot
                if not state.insideBoard(pos):  # TODO: maybe consider bots in the way,
                    break
                state.board[pos[0], pos[1]] = my_id  # paint the square

        state.loadPlayerScores()
        state.loadMoves()

        return state

    # given a numpy array, get the ratio of my cells over the others in that matrix, if all mine return 0
    def matrixRatio(self, matrix):
        not_mine = np.count_nonzero(matrix != self.me)  # count not mine
        empty = np.count_nonzero(matrix == -1)  # count empty
        opponents = not_mine - empty
        total = max(matrix.size, 1)
        # explore more towards enemy cells than towards empty cells
        return 0.6 * (opponents / total) + 0.4 * (empty / total)

    # return true if the given matrix is all mine
    def matrixMine(self, matrix):
        return np.count_nonzero(matrix == self.me) == matrix.size

    # given a direction return the submatrix of the corresponding quadrante
    def getSubmatrixFromDirection(self, direction):
        x, y = self.players[self.me]  # get current coords
        if direction[0] == -1:  # up
            if direction[1] == -1:  # left
                return self.board[:x + 1, :y + 1]
            if direction[1] == 0:  # same column
                return self.board[:x + 1, :]
            if direction[1] == 1:  # right
                return self.board[:x + 1, y:]
        elif direction[0] == 0:  # same row
            if direction[1] == -1:  # left
                return self.board[:, :y + 1]
            if direction[1] == 1:  # right
                return self.board[:, y:]
        elif direction[0] == 1:  # down
            if direction[1] == -1:  # left
                return self.board[x:, :y + 1]
            if direction[1] == 0:  # same column
                return self.board[x:, :]
            if direction[1] == 1:  # right
                return self.board[x:, y:]

    # given a history of states, return the winning player
    def getWinner(self):
        return max([x for x in self.scores if x > 0], key=lambda x: self.scores[x])

    # return true if i win at the current state
    def iWon(self):
        return self.me == self.getWinner()

    # singleton to return a random, not visited state (fails if none left) -> check before -> expand
    def getRandomMove(self):
        return self.moves.pop()

    # given the current state and another, return true if they have the same board and same player positions
    def hasChanged(self, state):
        return any(not np.array_equal(self.players[p], state.players[p]) for p in self.players) or not np.array_equal(self.board, state.board)

    # given the current state get the distance to the (one) opponent
    def getDistanceOpponent(self):
        other_players = list(players.keys())
        other_players.remove(self.me)
        return manhattan(players[self.me], players[other_players[0]])

    def __repr__(self):
        return "turns_left: %d\nplayers: %s,\nboard: %s" % (self.turns_left, self.players, self.board)


##################################################################

ROOT = None  # the search tree root


# very important: Node.state.moves holds all the moves that have not been expanded
# Node.children holds all the Nodes resulting from popping a move from Node.state.moves
class Node(object):
    MAX_DEPTH = 3  # max depth for the simulation
    C = 1.6  # learning rate, bigger means more exploration

    ID = 0

    def __init__(self, state, parent=None, move=None):
        self.wins = 0
        self.plays = 0
        self.parent = parent
        self.children = []
        self.state = state
        self.move = move  # the move made that led to this state
        self.expanded = False
        self.best_child = None  # node value evaluated by heuristic function for the board
        self.best_child_heuristic = 0
        self.best_child_score = 0
        self.unchanged = 0  # for how long is this the same state
        # debug stuff
        self.id = Node.ID
        Node.ID += 1
        if parent:
            self.depth = parent.depth + 1
            self.heuristic = self.getGreedyHeuristic()  # node value evaluated by heuristic function for the board
            self.improvement_accumulated = 0  # accumulated improvement for the best child (excluding this node)
            self.improvement = self.getImprovement()   # improvement from parent
            self.past_positions = parent.past_positions[:]  # list oof tuples (position, improvement) to use for redundancy clear
            self.past_positions.append((self.state.myPos(), self.improvement))
            # with Timer("backprop"):
            self.backprop(self.state.iWon())  # back prop if applicable
        else:  # for the root
            self.improvement = 0  # TODO remove after debug in updateBestChild print
            self.past_positions = []
            self.depth = 0

    def printRecursive(self):
        p(self)
        if self.best_child:
            self.getBestValueChild().printRecursive()

    # get the best move for the explored states from the current Node
    def getNextMove(self):
        if len(self.children) == 0:
            best = self.state.fixed_moves[0]  # random move, because they are shuffled
        elif self.unchanged >= 4:
            best = self.state.fixed_moves[0]
        elif self.unchanged >= 1:
            best = self.getBestShootExcept(self.getBestValueChild()).move
        else:
            # self.printRecursive()
            best = self.getBestValueChild().move  # my best child
        return best  # my move in that child, 0 indexed, IDS are not

    # get the best shoot from the current position
    def getBestShootExcept(self, ignore):
        all_shots_except = [child for child in self.children if child.move["type"] == "shoot" and not equal_move(child.move, ignore.move)]
        if len(all_shots_except) == 0:  # if there are no shots available
            all_shots_except = [child for child in self.children if not equal_move(child.move, ignore.move)]
        return max(all_shots_except, key=lambda c: c.getImprovement())

    # given a move, see if it is redundant for the current node
    def isMoveRedundant(self, move):
        # remove moves towards matrices I dominate
        if self.state.matrixMine(self.state.getSubmatrixFromDirection(move["direction"])):
            return True
        # remove shot if it is the same as before
        if self.move and self.move["type"] == "shot" == move["type"] and equal_dir(self.move, move):
            return True
        return False

    # given a node with past_positions, check if this node is redundant
    def isRedundant(self):
        # remove shots that get nothing
        if self.improvement == 0 and self.move["type"] == "shoot":
            return True
        # if this move is shoot (improvement=1) and the next is walk for the same dir
        if self.parent.move is not None and self.parent.move["type"] == "shoot" and self.move["type"] == "walk" and self.improvement == 1 and equal_dir(self.parent.move, self.move):
            return True
        # crop redundant nodes
        my_pos = self.state.myPos()
        for prev_pos, prev_imp in self.past_positions[:-1]:
            if prev_imp == 0 and prev_pos == prev_imp:
                return True
            elif prev_imp != 0:
                break
        return False

    def getImprovement(self):
        # return self.state.myScore() - self.parent.state.myScore()
        # heuristic_part = self.state.getDistanceOpponent() / self.state.rows + self.state.columns
        # return 0.9 * improvement_part - 0.1 * heuristic_part
        improvement_part = 0.8 * (self.state.myScore() - self.parent.state.myScore()) + 0.2 * max(0, (self.parent.state.opponentScore() - self.state.opponentScore()))
        return improvement_part

    # get the value (wins/plays ratio) for this node
    def getValue(self):
        return self.wins / self.plays

    # get the child with the highest Value
    # MUST be expanded, otherwise max fails for empty list
    def getBestValueChild(self):
        b = self.best_child
        if b.improvement == 0 and b.move["type"] == "shoot":
            b.move["type"] = "walk"
        # p("BEST: %s, improvement: %d, best: %d" % (b, b.improvement, b.getBestImprovement()))
        return b
        # return max(self.children, key=lambda c: c.heuristic)

    # get the child with the highest Value
    # MUST be expanded, otherwise max fails for empty list
    def getBestUCTChild(self):
        return max(self.children, key=lambda c: c.getUCT())

    # get the UCT value for the current node (the root node will crash due to no parent)
    def getUCT(self):
        return self.getValue() + Node.C * sqrt(log(self.parent.plays) / self.plays)

    # if this node is expanded keep on selection until a not expanded node is found
    def selection(self):
        # p("Selection for " + str(self) + "\n from these: " + str(self.children) + "\n i choose: ")
        if self.expanded:
            # p(self.getBestUCTChild())
            self.getBestUCTChild().selection()
        elif len(self.state.moves) > 0:
            # p("NONE -> i am simulating")
            self.run_simulation()

    # get the accumulated improvement of a node (current  + best_child)
    def getBestImprovement(self):
        return self.improvement + self.improvement_accumulated

    # expand an unexpanded node -> produce all children
    def expand(self):
        self.expanded = True
        for move in self.state.fixed_moves:
            if not self.isMoveRedundant(move):  # check if this move is not redundant
                new_child = Node(self.state.getNewStateAlone(move), self, move)
                if not new_child.isRedundant():  # check that the resulting child is not redundant
                    self.children.append(new_child)

    # average the heuristic of this node according to the child's heuristics through backtracking
    def updateBestChild(self, child):
        # p("updating best child for node %d" % self.id)
        if not self.best_child or child.getBestImprovement() > self.best_child_score or (child.getBestImprovement() == self.best_child_score and child.heuristic > self.best_child.heuristic):
            # p("Node %d(imp=%d) best child(%d): updated from %s, to %s (depth = %s)" % (self.id, self.improvement, child.id, self.best_child.getBestImprovement() if self.best_child else "null", child.getBestImprovement(), child.depth))
            self.best_child = child
            self.best_child_score = child.getBestImprovement()
            self.best_child_heuristic = child.heuristic  # TODO: meter no construtor do Node
            self.improvement_accumulated = self.best_child.getBestImprovement()

    # after a simulation, save the result and propagate it back to its parents recursively
    def backprop(self, victory, child=None):
        # p("backprop from %d" % child.id)
        self.plays += 1
        if victory:
            self.wins += 1
        if child:  # if a child called this function
            self.updateBestChild(child)
        if self.parent:
            # p("backprop from %d to %d with %d" % (self.id, self.parent.id, self.getBestImprovement()))
            self.parent.backprop(victory, self)

    # run a NEW simulation for the current node
    def run_simulation(self):
        # either simulate for turns_left or MAX_DEPTH
        node = self  # current node
        for i in range(min(self.state.turns_left, Node.MAX_DEPTH + 1)):  # depth of the simulation
            if len(node.state.moves) == 0:
                return  # all moves have been simulated
            random_move = node.state.getRandomMove()  # get a random move from the node
            node = Node(node.state.getNewStateAlone(random_move), node, random_move)

    # start mcts -> if expanded, select best child and run recursively
    # if not expanded -> expand and run_simulation
    def mcts(self):
        if self.expanded:
            self.selection()
        else:
            self.expand()

    # given the current game and a previous game, calculate the value of the current state
    def getGreedyHeuristic(self):
        s = self.state

        # get the ratio of cells different than me in the pointing direction
        sub_matrix = s.getSubmatrixFromDirection(self.move["direction"])
        ratio = s.matrixRatio(sub_matrix)

        return ratio  # * (sub_matrix.size / self.state.board.size)

    def __repr__(self):
        return "NODE %2d, depth: %d, improvement: %d, move: %s" % (self.id, self.depth, self.improvement, self.move)
        if self.plays > 0:
            return "NODE %d: wins/plays = %d/%d = %.1f, value = %d, UCT: %.2f, children = %d, unexplored = %d" % (self.id, self.wins, self.plays, (100 * self.wins / self.plays), self.getValue(), self.getUCT(), len(self.children), len(self.state.moves))
        else:
            return "NODE %d: wins/plays = %d/%d, children = %d, unexplored = %d" % (self.id, self.wins, self.plays, len(self.children), len(self.state.moves))


def concurrent_mcts():
    global ROOT
    while ROOT is None:
        pass
    while True:
        ROOT.mcts()

##################################################################


class Bot(object):

    def __init__(self, player_id):
        self.player_id = player_id
        thread.start_new_thread(concurrent_mcts, ())

    # make this move last as long as possible
    def smart_sleep(self, start):
        max_sleep = 0.44 - (default_timer() - start)
        sleep(max_sleep)
        # p("start = %.4fms, max_sleep = %.4fms, total: %.4fms, " % (start / 1000, max_sleep, (default_timer() - start)))

    def clearChoice(self, c):  # pragma: no cover
        try:
            c["direction"] = c["direction"].tolist()
        except:
            pass
        return c

    def load_root(self, s):
        global ROOT
        state = State(s, self.player_id)  # tipically 0.1 ms
        # if no root is set or if the previous state is not the same as the current
        if not ROOT or ROOT.state.hasChanged(state):
            ROOT = Node(state)
        else:
            ROOT.unchanged += 1
            ROOT.state.turns_left += 1

    def next_move(self, s):  # pragma: no cover
        # with Timer("next_move"):
        start = default_timer()
        self.load_root(s)
        self.smart_sleep(start)
        return self.clearChoice(ROOT.getNextMove())


if __name__ == "__main__":  # pragma: no cover
    multipaint.run(Bot)

